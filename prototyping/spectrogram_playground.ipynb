{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "\n",
    "# https://electronics.stackexchange.com/questions/251157/can-i-use-the-fft-to-recognize-musical-notes-on-a-piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import audiosegment\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageDraw\n",
    "from moviepy.editor import AudioFileClip, VideoFileClip\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "\n",
    "from signal_process_utils import generate_frequency_table\n",
    "\n",
    "# logger with special stream handling to output to stdout in Node.js\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "stdout_handler = logging.StreamHandler(sys.stdout)\n",
    "stdout_handler.setFormatter(logging.Formatter('%(asctime)s %(message)s'))\n",
    "stdout_handler.setLevel(logging.INFO)\n",
    "logger.addHandler(stdout_handler)\n",
    "\n",
    "\n",
    "class Decomposer(object):\n",
    "\n",
    "    def __init__(self, wav_file, plot=False, stop_time=None, debug=False):\n",
    "        \"\"\"\n",
    "        Class to decompose an wav file into its frequency vs. time spectrogram, and map that to piano keys.\n",
    "        Args:\n",
    "            wav_file (str): name of wav file to process.\n",
    "            plot (bool): for debugging.\n",
    "            debug (bool): for debugging.\n",
    "        \"\"\"\n",
    "        self.wav_file = wav_file\n",
    "        self.plot = plot\n",
    "        self.stop_time = stop_time\n",
    "        self.debug = debug\n",
    "\n",
    "        # audio/acoustic data\n",
    "        self.seg = audiosegment.from_file(wav_file)\n",
    "        self.raw_samples = np.array(self.seg.seg.get_array_of_samples())\n",
    "        self.freq_table = generate_frequency_table()\n",
    "\n",
    "        # init a fresh piano img (use HSV if not using addWeighted func in _generate_keyboard, else RGB)\n",
    "        piano_img = os.path.join('assets', 'piano.jpg')\n",
    "        self.piano_template = Image.open(piano_img).convert('RGBA')\n",
    "\n",
    "        # other useful values\n",
    "        self.max_freq = 4186  # hz of high c (key 88)\n",
    "        self.fps_out = 30  # fps of output video\n",
    "\n",
    "        # useful transform for better plotting visualization of spectrogram\n",
    "        self._log_amps = lambda x: 10 * np.log10(x + 1e-9)\n",
    "\n",
    "        def _find_nearest(value, array):\n",
    "            \"\"\"Quantize a value (detected frequency) to piano nearest frequency.\"\"\"\n",
    "            idx = np.argmin(np.abs(array - value))\n",
    "            return 89 - idx\n",
    "\n",
    "        self._map_freq2note = np.vectorize(partial(_find_nearest, array=self.freq_table['Frequency (Hz)'].values))\n",
    "\n",
    "    def cvt_audio_to_piano(self):\n",
    "        self._generate_spectrogram()\n",
    "        self._parse_spectrogram()\n",
    "        self._build_movie()\n",
    "\n",
    "    @staticmethod\n",
    "    def _median_filter(arr, length=5, stride=1):\n",
    "        \"\"\"Compute the 1D median filter of an array. This helps remove outliers and noise.\n",
    "\n",
    "        Args:\n",
    "            arr (np.ndarray): arr to filter\n",
    "            length (int): window size\n",
    "            stride (int): step size\n",
    "\n",
    "        Returns:\n",
    "            smoothed np.ndarray\n",
    "        \"\"\"\n",
    "        nrows = ((arr.size - length) // stride) + 1\n",
    "        n = arr.strides[0]\n",
    "        windowed_matrix = np.lib.stride_tricks.as_strided(arr, shape=(nrows, length), strides=(stride * n, n))\n",
    "        median = np.median(windowed_matrix, axis=1)\n",
    "        arr[-median.shape[0]:] = median\n",
    "        return arr\n",
    "\n",
    "    def _generate_spectrogram(self, window_length_s=0.5, overlap=0.9):\n",
    "        \"\"\" Generate a spectrogram from our wav data.\"\"\"\n",
    "        self.freqs, self.times, self.amplitudes = self.seg.spectrogram(window_length_s=window_length_s,\n",
    "                                                                       overlap=overlap)\n",
    "        # slice out only the ranges we are interested in\n",
    "        max_freq_idx = np.argmin(self.freqs < self.max_freq)\n",
    "        self.times = self.times / 2\n",
    "        self.freqs = self.freqs[:max_freq_idx]\n",
    "        self.amplitudes = self.amplitudes[:max_freq_idx, :]\n",
    "\n",
    "        # median filter along time axis to get rid of white noise\n",
    "        self.amplitudes = np.apply_along_axis(self._median_filter, 1, self.amplitudes)\n",
    "\n",
    "        if self.stop_time:\n",
    "            self.t_final = np.where(self.times < self.stop_time)[0][-1]\n",
    "        else:\n",
    "            self.t_final = self.times.shape[0]\n",
    "\n",
    "        self._plot_spectrogram(self.amplitudes, 'Raw Spectrogram')\n",
    "\n",
    "    def _parse_spectrogram(self):\n",
    "        \"\"\" Parse the spectrogram by iterating through the time axis, thresholding\n",
    "        away quiet frequencies, and mapping the dominant frequencies to piano keys.\"\"\"\n",
    "\n",
    "        def _get_peaks_and_threshold(t):\n",
    "            \"\"\" Given a time(t), extract the dominant frequencies in the amplitude\n",
    "            matrix using argrelextrema, and threshold all other values to zero.\n",
    "            Store in a new matrix, self.dominant_amplitudes. Thresholding performed by\n",
    "            selecting the inverse indices of the detected peaks.\n",
    "            \"\"\"\n",
    "\n",
    "            # peak detection in a amplitude vector at time t\n",
    "            # take log of vec since amplitudes decay exponentially at higher freqs\n",
    "            # https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy/52612432#52612432\n",
    "            peaks_idx = find_peaks(np.log(self.amplitudes[:, t]), prominence=3)[0]\n",
    "\n",
    "            # select inverse indices of peaks, threshold to zero\n",
    "            arr = self.dominant_amplitudes[:, t]\n",
    "            ia = np.indices(arr.shape)\n",
    "            not_indices = np.setxor1d(ia, peaks_idx)\n",
    "            arr[not_indices] = 0\n",
    "            self.dominant_amplitudes[:, t] = arr\n",
    "\n",
    "        def _get_notes(t):\n",
    "            \"\"\" Map the dominant frequencies at time(t) to the corresponding piano keys.\n",
    "\n",
    "            If we detect a frequency in self.dominant_amplitudes, quantize that frequency into\n",
    "            one of the piano frequency bins, store in array detected_freqs, and get the piano note\n",
    "            index using self._map_freq2note.\"\"\"\n",
    "\n",
    "            # if dominant frequency vector is non-zero, map all detected freqs to notes\n",
    "            amp_arr = self.dominant_amplitudes[:, t]\n",
    "            if np.count_nonzero(amp_arr) != 0:\n",
    "                freq_idx = np.nonzero(amp_arr)[0]\n",
    "                amp_arr_nonzero = amp_arr[freq_idx]\n",
    "                detected_freqs = self.freqs[freq_idx]\n",
    "                f_table_idx = self._map_freq2note(detected_freqs)\n",
    "\n",
    "                if self.debug:\n",
    "                    # bc dataframes are prettier than matrices\n",
    "                    helmholtz = self.freq_table.iat[90 - f_table_idx].Helmholtzname\n",
    "                    notes_out = pd.DataFrame([\n",
    "                        helmholtz.values,\n",
    "                        detected_freqs,\n",
    "                        amp_arr_nonzero,\n",
    "                        f_table_idx]\n",
    "                    ).T\n",
    "                    notes_out.columns = ['note', 'f', 'db', 'num']\n",
    "                    print(notes_out)\n",
    "\n",
    "                return f_table_idx, amp_arr_nonzero\n",
    "            return None, None\n",
    "\n",
    "        # init keyboard frames with predefined shape - will eventually turn into video\n",
    "        keyboard_img_size = [self.t_final] + [240, 1920, 3]\n",
    "        self.keyboard_frames = np.empty(keyboard_img_size, dtype=np.uint8)\n",
    "\n",
    "        # init dom freqs matrix, iterate through time, find peaks and threshold\n",
    "        self.dominant_amplitudes = self.amplitudes.copy()\n",
    "        for t in tqdm(range(self.t_final)):\n",
    "            _get_peaks_and_threshold(t)\n",
    "        logger.info('[Decomposer] >>>> Found dominant frequencies.')\n",
    "\n",
    "        # median filter along time axis to get rid of white noise\n",
    "        self.dominant_amplitudes = np.apply_along_axis(self._median_filter, 1, self.dominant_amplitudes)\n",
    "\n",
    "        # iterate through time, map dominant frequencies to notes, generate keyboard visualizations\n",
    "        for t in tqdm(range(self.t_final)):\n",
    "            t_note_data = _get_notes(t)\n",
    "            keyboard = self._generate_keyboard(*t_note_data)\n",
    "            self.keyboard_frames[t, ...] = keyboard\n",
    "        logger.info('[Decomposer] >>>> Mapped frequencies to notes and generated keyboard visualizations.')\n",
    "\n",
    "        self._plot_spectrogram(self.dominant_amplitudes, 'Filtered Spectrogram of Dominant Frequencies')\n",
    "\n",
    "    def _generate_keyboard(self, f_table_idx, amp_arr_nonzero, loudness_thresh=0.25):\n",
    "        \"\"\"\n",
    "        Iterate through notes found in sample and draw on keyboard image.\n",
    "        Intensity of color depends on loudness (decibels).\n",
    "        All detected notes are stacked into a single image.\n",
    "\n",
    "        Args:\n",
    "            f_table_idx (np.ndarray): indices of active notes in self.freq_table.\n",
    "            amp_arr_nonzero (np.ndarray): vector containing raw amplitude values\n",
    "            loudness_thresh (float): [0, 1] that loudness value must exceed to be drawn to keyboard\n",
    "        Returns:\n",
    "            np.array image of colorized piano\n",
    "\n",
    "        \"\"\"\n",
    "        piano_out = self.piano_template.copy()\n",
    "        if f_table_idx is not None:\n",
    "            amp_arr_nonzero /= np.max(amp_arr_nonzero)  # normalize vector [0, 1]\n",
    "            # iterate through detected notes, extract location on keyboard if loudness thresh met\n",
    "            for n in range(f_table_idx.shape[0]):\n",
    "                idx = f_table_idx[n]\n",
    "                loudness = amp_arr_nonzero[n]\n",
    "\n",
    "                if loudness > loudness_thresh:\n",
    "                    piano_loc_points = self.freq_table.iat[89 - idx, -1]\n",
    "                    if type(piano_loc_points) is not list: continue  # handle nan case\n",
    "\n",
    "                    # color in detected note on keyboard img, stack onto output img\n",
    "                    poly = Image.new('RGBA', (1920, 240))\n",
    "                    pdraw = ImageDraw.Draw(poly)\n",
    "                    pdraw.polygon(piano_loc_points,\n",
    "                                  fill=(0, 255, 0, int(255 * loudness)),\n",
    "                                  outline=(0, 255, 240, 255))\n",
    "                    piano_out.paste(poly, mask=poly)\n",
    "        return np.array(piano_out.convert('RGB'))\n",
    "\n",
    "    def _plot_spectrogram(self, amplitude_matrix, title=''):\n",
    "        \"\"\" Plot our spectrograms for debugging. \"\"\"\n",
    "        if self.plot:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.pcolormesh(self.times, self.freqs, self._log_amps(amplitude_matrix))\n",
    "            plt.xlabel(\"Time in Seconds\")\n",
    "            plt.ylabel(\"Frequency in Hz\")\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "\n",
    "    def _build_movie(self):\n",
    "        \"\"\" Concatenate self.keyboard_frames images into video file, add back original music.\"\"\"\n",
    "        fps = 1. / (self.times[1] - self.times[0])\n",
    "\n",
    "        imageio.mimwrite('tmp.mp4', self.keyboard_frames, fps=fps)\n",
    "\n",
    "        outname = self.wav_file.replace('input', 'output')\n",
    "        outname = outname.replace('wav', 'mp4')\n",
    "\n",
    "        output = VideoFileClip('tmp.mp4')\n",
    "        output = output.set_audio(AudioFileClip(self.wav_file))\n",
    "        output.write_videofile(\n",
    "            outname,\n",
    "            fps=self.fps_out,\n",
    "            temp_audiofile=\"temp-audio.m4a\",\n",
    "            remove_temp=True,\n",
    "            codec=\"libx264\",\n",
    "            audio_codec=\"aac\"\n",
    "        )\n",
    "        os.remove('tmp.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# !ffmpeg -y -i output/4_6Fz3RCZPQ.mp4 -f mp3 -ab 192000 -vn input/4_6Fz3RCZPQ.mp3\n",
    "# !ffmpeg -y -i input/4_6Fz3RCZPQ.mp3 -acodec pcm_u8 -ar 22050 input/4_6Fz3RCZPQ.wav\n",
    "song_file = os.path.join('input', '4_6Fz3RCZPQ.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Decomposer(song_file, False, 20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/794 [00:00<?, ?it/s]WARNING:py.warnings:/Users/mnalavadi/anaconda3/envs/cv/lib/python3.6/site-packages/ipykernel/__main__.py:124: RuntimeWarning: divide by zero encountered in log\n",
      "\n",
      "100%|██████████| 794/794 [00:00<00:00, 3473.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26 18:09:11,162 [Decomposer] >>>> Found dominant frequencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:__main__:[Decomposer] >>>> Found dominant frequencies.\n",
      "100%|██████████| 794/794 [00:06<00:00, 128.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-01-26 18:09:19,142 [Decomposer] >>>> Mapped frequencies to notes and generated keyboard visualizations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:__main__:[Decomposer] >>>> Mapped frequencies to notes and generated keyboard visualizations.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output/4_6Fz3RCZPQ.mp4\n",
      "[MoviePy] Writing audio in temp-audio.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8671/8671 [00:09<00:00, 963.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] Writing video output/4_6Fz3RCZPQ.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 596/596 [00:01<00:00, 308.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output/4_6Fz3RCZPQ.mp4 \n",
      "\n",
      "CPU times: user 20.1 s, sys: 3.82 s, total: 23.9 s\n",
      "Wall time: 33.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "self._generate_spectrogram()\n",
    "self._parse_spectrogram()\n",
    "self._build_movie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.raw_samples.shape #17340192 / 37747354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !open output/4_6Fz3RCZPQ.mp4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak detection debugging\n",
    "\n",
    "# from scipy.signal import find_peaks\n",
    "# for t in range(200): #range(self.times.shape[0]):\n",
    "#     amp_arr = self.amplitudes[:, t]\n",
    "#     if not np.sum(amp_arr) == 0*len(amp_arr):\n",
    "#         log_amp_arr = np.log(amp_arr)\n",
    "#         peaks_idx = find_peaks(log_amp_arr, prominence=3)[0]\n",
    "#         print(peaks_idx)\n",
    "#         amp_arr[peaks_idx]\n",
    "#         plt.figure(figsize=(20, 4))\n",
    "#         plt.plot(range(len(amp_arr)), amp_arr)\n",
    "#         plt.scatter(peaks_idx, amp_arr[peaks_idx], s=12, c='r')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_notes(t):\n",
    "    \"\"\" Map the dominant frequencies at time(t) to the corresponding piano keys.\n",
    "\n",
    "    If we detect a frequency in self.dominant_amplitudes, quantize that frequency into\n",
    "    one of the piano frequency bins, store in array detected_freqs, and get the piano note\n",
    "    index using self._map_freq2note.\"\"\"\n",
    "\n",
    "    # if dominant frequency vector is non-zero, map all detected freqs to notes\n",
    "    amp_arr = self.dominant_amplitudes[:, t]\n",
    "    if np.count_nonzero(amp_arr) != 0:\n",
    "        freq_idx = np.nonzero(amp_arr)[0]\n",
    "        amp_arr_nonzero = amp_arr[freq_idx]\n",
    "        detected_freqs = self.freqs[freq_idx]\n",
    "        f_table_idx = self._map_freq2note(detected_freqs)\n",
    "        return f_table_idx, amp_arr_nonzero\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f  self._generate_keyboard self._generate_keyboard(*_get_notes(600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.imshow(self._generate_keyboard(*_get_notes(600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -i input/4_6Fz3RCZPQ.mp3 -acodec pcm_u8 -ar 22050 input/4_6Fz3RCZPQ.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "\n",
    "sample_rate, samples = wavfile.read('input/4_6Fz3RCZPQ.wav')\n",
    "frequencies, times, spectrogram = signal.spectrogram(samples, sample_rate)\n",
    "\n",
    "plt.imshow(spectrogram)\n",
    "plt.pcolormesh(times, frequencies, spectrogram)\n",
    "plt.ylabel('Frequency [Hz]')\n",
    "plt.xlabel('Time [sec]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "%matplotlib inline\n",
    "%load_ext line_profiler\n",
    "\n",
    "song_file = os.path.join('input', 'twinkle.mp3')\n",
    "# https://electronics.stackexchange.com/questions/251157/can-i-use-the-fft-to-recognize-musical-notes-on-a-piano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "from functools import partial\n",
    "\n",
    "import audiosegment\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from moviepy.editor import *\n",
    "from scipy.signal import find_peaks\n",
    "from tqdm import tqdm\n",
    "\n",
    "from signal_process_utils import generate_frequency_table\n",
    "\n",
    "\n",
    "class Decomposer:\n",
    "\n",
    "    def __init__(self, mp3_file, plot=False, stop_time=None, debug=False):\n",
    "        \"\"\"\n",
    "        Class to decompose an mp3 file into its frequency vs. time spectrogram, and map that to piano keys.\n",
    "        Args:\n",
    "            mp3_file (str): name of mp3 file to process.\n",
    "            plot (bool): for debugging.\n",
    "            debug (bool): for debugging.\n",
    "        \"\"\"\n",
    "        self.mp3_file = mp3_file\n",
    "        self.plot = plot\n",
    "        self.stop_time = stop_time\n",
    "        self.debug = debug\n",
    "\n",
    "        # audio/acoustic data\n",
    "        self.seg = audiosegment.from_file(mp3_file)\n",
    "        self.raw_samples = np.array(self.seg.seg.get_array_of_samples())\n",
    "        self.freq_table = generate_frequency_table()\n",
    "        \n",
    "        # init a fresh piano img (use HSV if not using addWeighted func in _generate_keyboard)\n",
    "        piano_img = os.path.join('assets', 'piano.jpg')\n",
    "        self.piano_template = cv2.cvtColor(cv2.imread(piano_img), cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # other useful values\n",
    "        self.in_sample_rate = self.seg.frame_rate\n",
    "        self.num_samples = len(self.raw_samples)\n",
    "        self.ms = len(self.seg)\n",
    "        self.max_freq = 4186  # hz of high c (key 88)\n",
    "\n",
    "        # useful transform for better plotting visualization of spectrogram\n",
    "        self._log_amps = lambda x: 10 * np.log10(x + 1e-9)\n",
    "\n",
    "        def _find_nearest(value, array):\n",
    "            \"\"\"Quantize a value (detected frequency) to piano nearest frequency.\"\"\"\n",
    "            idx = np.argmin(np.abs(array - value))\n",
    "            return 89 - idx\n",
    "\n",
    "        self._map_freq2note = np.vectorize(partial(_find_nearest, array=self.freq_table['Frequency (Hz)'].values))\n",
    "\n",
    "    def cvt_mp3_to_piano(self):\n",
    "        self._generate_spectrogram()\n",
    "        self._parse_spectrogram()\n",
    "        self._build_movie()\n",
    "\n",
    "    @staticmethod\n",
    "    def _median_filter(arr, length=5, stride=1):\n",
    "        \"\"\"Compute the 1D median filter of an array. This helps remove outliers and noise.\n",
    "\n",
    "        Args:\n",
    "            arr (np.ndarray): arr to filter\n",
    "            length (int): window size\n",
    "            stride (int): step size\n",
    "\n",
    "        Returns:\n",
    "            smoothed np.ndarray\n",
    "        \"\"\"\n",
    "        nrows = ((arr.size - length) // stride) + 1\n",
    "        n = arr.strides[0]\n",
    "        windowed_matrix = np.lib.stride_tricks.as_strided(arr, shape=(nrows, length), strides=(stride * n, n))\n",
    "        median = np.median(windowed_matrix, axis=1)\n",
    "        arr[-median.shape[0]:] = median\n",
    "        return arr\n",
    "\n",
    "    def _generate_spectrogram(self, window_length_s=0.5, overlap=0.9):\n",
    "        \"\"\" Generate a spectrogram from our mp3 data.\"\"\"\n",
    "        self.freqs, self.times, self.amplitudes = self.seg.spectrogram(window_length_s=window_length_s,\n",
    "                                                                       overlap=overlap)\n",
    "        # slice out only the ranges we are interested in\n",
    "        max_freq_idx = np.argmin(self.freqs < self.max_freq)\n",
    "        self.times = self.times / 2\n",
    "        self.freqs = self.freqs[:max_freq_idx]\n",
    "        self.amplitudes = self.amplitudes[:max_freq_idx, :]\n",
    "\n",
    "        # median filter along time axis to get rid of white noise\n",
    "        self.amplitudes = np.apply_along_axis(self._median_filter, 1, self.amplitudes)\n",
    "        \n",
    "        if self.stop_time:\n",
    "            self.t_final = np.where(self.times < self.stop_time)[0][-1]\n",
    "        else:\n",
    "            self.t_final = self.times.shape[0]\n",
    "\n",
    "        self._plot_spectrogram(self.amplitudes, 'Raw Spectrogram')\n",
    "\n",
    "    def _parse_spectrogram(self):\n",
    "        \"\"\" Parse the spectrogram by iterating through the time axis, thresholding\n",
    "        away quiet frequencies, and mapping the dominant frequencies to piano keys.\"\"\"\n",
    "\n",
    "        def _get_peaks_and_threshold(t):\n",
    "            \"\"\" Given a time(t), extract the dominant frequencies in the amplitude\n",
    "            matrix using argrelextrema, and threshold all other values to zero the\n",
    "            values a new matrix, self.dominant_amplitudes. Thresholding performed by\n",
    "            selecting the inverse indices of the detected peaks.\n",
    "            \"\"\"\n",
    "\n",
    "            # peak detection in a amplitude vector at time t\n",
    "            # take log of vec since amplitudes decay exponentially at higher freqs\n",
    "            # https://stackoverflow.com/questions/1713335/peak-finding-algorithm-for-python-scipy/52612432#52612432\n",
    "            peaks_idx = find_peaks(np.log(self.amplitudes[:, t]), prominence=3)[0]\n",
    "\n",
    "            # select inverse indices of peaks, threshold to zero\n",
    "            arr = self.dominant_amplitudes[:, t]\n",
    "            ia = np.indices(arr.shape)\n",
    "            not_indices = np.setxor1d(ia, peaks_idx)\n",
    "            arr[not_indices] = 0\n",
    "            self.dominant_amplitudes[:, t] = arr\n",
    "\n",
    "        def _get_notes(t):\n",
    "            \"\"\" Map the dominant frequencies at time(t) to the corresponding piano keys.\n",
    "\n",
    "            If we detect a frequency in self.dominant_amplitudes, quantize that frequency into\n",
    "            one of the piano frequency bins, store in array detected_freqs, and get the piano note\n",
    "            index using self._map_freq2note.\"\"\"\n",
    "\n",
    "            # if dominant frequency vector is non-zero, map all detected freqs to notes\n",
    "            amp_arr = self.dominant_amplitudes[:, t]\n",
    "            if np.count_nonzero(amp_arr) != 0:\n",
    "                freq_idx = np.nonzero(amp_arr)[0]\n",
    "                amp_arr_nonzero = amp_arr[freq_idx]\n",
    "                detected_freqs = self.freqs[freq_idx]\n",
    "                f_table_idx = self._map_freq2note(detected_freqs)\n",
    "\n",
    "                if self.debug:\n",
    "                    # bc dataframes are prettier than matrices\n",
    "                    helmholtz = self.freq_table.iat[90 - f_table_idx].Helmholtzname\n",
    "                    notes_out = pd.DataFrame([\n",
    "                        helmholtz.values,\n",
    "                        detected_freqs,\n",
    "                        amp_arr_nonzero,\n",
    "                        f_table_idx]\n",
    "                    ).T\n",
    "                    notes_out.columns = ['note', 'f', 'db', 'num']\n",
    "                    print(notes_out)\n",
    "\n",
    "                return f_table_idx, amp_arr_nonzero\n",
    "            return None, None\n",
    "\n",
    "        # init keyboard frames with predefined shape - will eventuall turn to video\n",
    "        keyboard_img_size = [self.t_final] + [232, 1910, 3]\n",
    "        self.keyboard_frames = np.empty(keyboard_img_size, dtype=np.uint8)\n",
    "\n",
    "        # init dom freqs matrix, iterate through time, find peaks and threshold\n",
    "        self.dominant_amplitudes = self.amplitudes.copy()\n",
    "        for t in tqdm(range(self.t_final)):\n",
    "            _get_peaks_and_threshold(t)\n",
    "        print('[Decomposer] >>>> Found dominant frequencies.')\n",
    "\n",
    "        # median filter along time axis to get rid of white noise\n",
    "        self.dominant_amplitudes = np.apply_along_axis(self._median_filter, 1, self.dominant_amplitudes)\n",
    "\n",
    "        # iterate through time, map dominant frequencies to notes, generate keyboard visualizations\n",
    "        for t in tqdm(range(self.t_final)):\n",
    "            t_note_data = _get_notes(t)\n",
    "            keyboard = self._generate_keyboard(*t_note_data)\n",
    "            self.keyboard_frames[t, ...] = keyboard\n",
    "        print('[Decomposer] >>>> Mapped frequencies to notes and generated keyboard visualizations.')\n",
    "\n",
    "        self._plot_spectrogram(self.dominant_amplitudes, 'Filtered Spectrogram of Dominant Frequencies')\n",
    "\n",
    "    def _generate_keyboard(self, f_table_idx, amp_arr_nonzeroloudness_thresh=0.25):\n",
    "        \"\"\"\n",
    "        Iterate through notes found in sample and draw on keyboard image.\n",
    "        Intensity of color depends on loudness (decibels).\n",
    "        All detected notes are stacked into a single image.\n",
    "\n",
    "        Args:\n",
    "            f_table_idx (np.ndarray): indices of active notes in self.freq_table.\n",
    "            amp_arr_nonzero (np.ndarray): vector containing raw amplitude values\n",
    "            loudness_thresh (float): [0, 1] that loudness value must exceed to be drawn to keyboard\n",
    "        Returns:\n",
    "            np.array image of colorized piano\n",
    "\n",
    "        \"\"\"\n",
    "        piano_out = self.piano_template.copy()\n",
    "        if f_table_idx is not None:\n",
    "            amp_arr_nonzero /= np.max(amp_arr_nonzero)  # normalize vector [0, 1]\n",
    "            # iterate through detected notes, extract location on keyboard if loudness thresh met\n",
    "            for n in range(f_table_idx.shape[0]):\n",
    "                idx = f_table_idx[n]\n",
    "                loudness = amp_arr_nonzero[n]\n",
    "                \n",
    "                if loudness > loudness_thresh:\n",
    "                    piano_loc_points = self.freq_table.iat[89 - idx, -1]\n",
    "                    if type(piano_loc_points) is not list: continue  # handle nan case\n",
    "\n",
    "                    # color in detected note on keyboard img, stack onto output img\n",
    "                    points = np.array(piano_loc_points, dtype=np.int32)\n",
    "                    cv2.fillPoly(piano_out, [points[:, ::-1]], [60, 255*loudness, 255]) \n",
    "        return cv2.cvtColor(piano_out, cv2.COLOR_HSV2RGB)\n",
    "# old method...\n",
    "#                         piano = self.piano_template.copy()\n",
    "#                     cv2.fillPoly(piano, [points[:, ::-1]], [0, 255, 0])\n",
    "#                     piano_out = cv2.addWeighted(piano_out, 1 - loudness, piano, loudness, 0)\n",
    "#         return piano_out\n",
    "\n",
    "\n",
    "    def _plot_spectrogram(self, amplitude_matrix, title=''):\n",
    "        \"\"\" Plot our spectrograms. \"\"\"\n",
    "        if self.plot:\n",
    "            plt.figure(figsize=(20, 6))\n",
    "            plt.pcolormesh(self.times, self.freqs, self._log_amps(amplitude_matrix))\n",
    "            plt.xlabel(\"Time in Seconds\")\n",
    "            plt.ylabel(\"Frequency in Hz\")\n",
    "            plt.title(title)\n",
    "            plt.show()\n",
    "\n",
    "    def _build_movie(self):\n",
    "        \"\"\" Concatenate self.keyboard_frames images into video file, add back original music.\"\"\"\n",
    "        frames = []\n",
    "        duration = self.times[1] - self.times[0]\n",
    "\n",
    "        for i in tqdm(range(self.keyboard_frames.shape[0])):\n",
    "            piano_img = self.keyboard_frames[i, ...]\n",
    "            single_frame = ImageClip(piano_img).set_duration(duration)\n",
    "            frames.append(single_frame)\n",
    "        out = concatenate_videoclips(frames, method=\"compose\")\n",
    "        out = out.set_audio(AudioFileClip(self.mp3_file))\n",
    "\n",
    "        # use temp audio to deal with moviepy bug\n",
    "        outfile = self.mp3_file.replace('input', 'output')\n",
    "        outfile = outfile.replace('mp3', 'mp4')\n",
    "        out.write_videofile(outfile,\n",
    "                            fps=30,\n",
    "                            temp_audiofile=\"temp-audio.m4a\",\n",
    "                            remove_temp=True,\n",
    "                            codec=\"libx264\",\n",
    "                            audio_codec=\"aac\"\n",
    "                            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "self = Decomposer(song_file, False, 50, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "self._generate_spectrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 1887/1887 [00:00<00:00, 2415.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Decomposer] >>>> Found dominant frequencies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1887/1887 [00:08<00:00, 217.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Decomposer] >>>> Mapped frequencies to notes and generated keyboard visualizations.\n"
     ]
    }
   ],
   "source": [
    "self._parse_spectrogram()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████| 1887/1887 [00:00<00:00, 60410.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output\\twinkle.mp4\n",
      "[MoviePy] Writing audio in temp-audio.m4a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1047/1047 [00:01<00:00, 851.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] Writing video output\\twinkle.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 1416/1416 [00:31<00:00, 45.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output\\twinkle.mp4 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "self._build_movie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peak detection debugging\n",
    "\n",
    "# from scipy.signal import find_peaks\n",
    "# for t in range(200): #range(self.times.shape[0]):\n",
    "#     amp_arr = self.amplitudes[:, t]\n",
    "#     if not np.sum(amp_arr) == 0*len(amp_arr):\n",
    "#         log_amp_arr = np.log(amp_arr)\n",
    "#         peaks_idx = find_peaks(log_amp_arr, prominence=3)[0]\n",
    "#         print(peaks_idx)\n",
    "#         amp_arr[peaks_idx]\n",
    "#         plt.figure(figsize=(20, 4))\n",
    "#         plt.plot(range(len(amp_arr)), amp_arr)\n",
    "#         plt.scatter(peaks_idx, amp_arr[peaks_idx], s=12, c='r')\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_notes(t):\n",
    "    \"\"\" Map the dominant frequencies at time(t) to the corresponding piano keys.\n",
    "\n",
    "    If we detect a frequency in self.dominant_amplitudes, quantize that frequency into\n",
    "    one of the piano frequency bins, store in array detected_freqs, and get the piano note\n",
    "    index using self._map_freq2note.\"\"\"\n",
    "\n",
    "    # if dominant frequency vector is non-zero, map all detected freqs to notes\n",
    "    amp_arr = self.dominant_amplitudes[:, t]\n",
    "    if np.count_nonzero(amp_arr) != 0:\n",
    "        freq_idx = np.nonzero(amp_arr)[0]\n",
    "        amp_arr_nonzero = amp_arr[freq_idx]\n",
    "        detected_freqs = self.freqs[freq_idx]\n",
    "        f_table_idx = self._map_freq2note(detected_freqs)\n",
    "\n",
    "        if self.debug:\n",
    "            # bc dataframes are prettier than matrices\n",
    "            helmholtz = self.freq_table.iat[90 - f_table_idx].Helmholtzname\n",
    "            notes_out = pd.DataFrame([\n",
    "                helmholtz.values,\n",
    "                detected_freqs,\n",
    "                amp_arr_nonzero,\n",
    "                f_table_idx]\n",
    "            ).T\n",
    "            notes_out.columns = ['note', 'f', 'db', 'num']\n",
    "            print(notes_out)\n",
    "\n",
    "        return f_table_idx, amp_arr_nonzero\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f  self._generate_keyboard self._generate_keyboard(*_get_notes(600))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 4))\n",
    "plt.imshow(self._generate_keyboard(*_get_notes(600)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.getsizeof(np.empty([1887, 232, 1910, 3], dtype=np.uint8)) / 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.zeros((40, 40, 3), np.uint8)\n",
    "img[:, :, 1] = 255\n",
    "# img[:, :, 2] = 100\n",
    "img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "print(img[0, 0, :])\n",
    "img[:, :, 1] = 1\n",
    "img = cv2.cvtColor(img, cv2.COLOR_HSV2RGB)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
